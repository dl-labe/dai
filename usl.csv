"#!/usr/bin/env python
# coding: utf-8

# In[22]:


import pandas as pd
import numpy as numpy
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_curve
from sklearn import metrics
import scipy.cluster.hierarchy as sch


# In[2]:


data = pd.read_csv('StudentsPerformance.csv')
data


# In[3]:


encoder = LabelEncoder()
data['gender'] = encoder.fit_transform(data['gender'])
data['race/ethnicity'] = encoder.fit_transform(data['race/ethnicity'])
data['parental level of education'] = encoder.fit_transform(data['parental level of education'])
data['lunch'] = encoder.fit_transform(data['lunch'])
data['test preparation course'] = encoder.fit_transform(data['test preparation course'])
data


# # K-Means Clustering

# In[4]:


X = data[['math score', 'reading score', 'writing score']]
X = (X - X.mean()) / X.std()


# In[5]:


wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()


# In[6]:


kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)
pred_y = kmeans.fit_predict(X)


# In[7]:


data['cluster'] = pred_y
plt.scatter(X.iloc[:,0], X.iloc[:,1], c=pred_y)
plt.xlabel('Math Score')
plt.ylabel('Reading Score')
plt.show()


# In[8]:


y_true = data[""gender""]
y_pred = kmeans.labels_
print(classification_report(y_true, y_pred))


# In[9]:


labels = data['cluster'].values
probas = kmeans.transform(X)
pos_proba = probas[:, 1]

fpr, tpr, thresholds = roc_curve(labels, pos_proba, pos_label=1)

plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc=""lower right"")
plt.show()


# # Hierarchial Clustering

# In[10]:


X = data[[""math score"", ""reading score"", ""writing score""]]


# In[11]:


X_norm = (X - X.mean()) / X.std()


# In[12]:


hierarchical = AgglomerativeClustering(n_clusters=2).fit(X_norm)


# In[13]:


y_true = data[""gender""]
y_pred = hierarchical.labels_
print(classification_report(y_true, y_pred))


# In[14]:


labels = hierarchical.labels_

plt.scatter(X[""math score""], X[""reading score""], c=labels)
plt.xlabel(""Math Score"")
plt.ylabel(""Reading Score"")
plt.show()


# # KNN

# In[15]:


X = data.drop('gender',axis=1)
y = data[""gender""]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# In[16]:


k = 5
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)


# In[17]:


y_pred = knn.predict(X_test)


# In[18]:


print(classification_report(y_test, y_pred))

"
